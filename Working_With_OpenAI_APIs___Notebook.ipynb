{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SumaOladri/potential-waddle/blob/master/Working_With_OpenAI_APIs___Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Working With OpenAI APIs**\n",
        "\n",
        "In this session, we will extensively use the `chat.completions()` API endpoints. The API allows generation of one-off responses (single-turn)and conducting chats/conversations (multi-turn), respectively.\n",
        "\n",
        "In this notebook, we will perform the following:\n",
        "1. Make API calls to the `chat.completions()` endpoints\n",
        "2. Perform prompt engineering and make prompts more nuanced to perform complex tasks\n",
        "3. Create a very simple 'AI Tutor' using the `chat.completions()` endpoint\n",
        "4. Measure the cost of making API calls via tokens and put guardrails in place to monitor and control costs\n",
        "<br>\n",
        "\n",
        "You can refer to more information about the ChatCompletions API from [OpenAI's API documentation](https://platform.openai.com/docs/guides/text-generation/chat-completions-api)."
      ],
      "metadata": {
        "id": "LxBP0R-EzDrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started with the `chat.completions()` API in Python\n",
        "\n",
        "We first need to install the `openai` Python library.\n",
        "\n",
        "In Google colab, you can install libraries by adding an exclamation mark before pip, i.e.\n",
        "\n",
        "`!pip install <library_name>`\n",
        "\n",
        "Before you proceed, you also need to get an OpenAI API key.\n",
        "\n",
        "Create an OpenAI account by referring to the [documentation](https://docs.google.com/document/d/e/2PACX-1vS3k_E9tUjM6HSZfXwZ02ydDYw0SNx41ZSfAzX4T7FA9lSLDz0anYMX0ZLzYXds5P6B0xNC_sT7Ggww/pub) and [get an API key here](https://platform.openai.com/account/api-keys)."
      ],
      "metadata": {
        "id": "yX178PpbSUQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install openai\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "moJyTKPwzGay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3e9cbf4-6d0f-45a6-ee5b-32fcfaa58c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use OpenAI APIs we need to set an API key (you can do that [here](https://platform.openai.com/account/api-keys)).\n",
        "\n",
        "In this instance, I have stored my API key in a file named \"OpenAI_API_Key.txt\" which is stored on my Google drive."
      ],
      "metadata": {
        "id": "9SMoW69ewQ6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Google Colab?\n",
        "\n",
        "[Google Colab](https://colab.research.google.com/) is a cloud-based platform provided by Google that allows users to write and execute Python code in a collaborative environment. It offers several benefits such as:\n",
        "- **Free Access to GPU and TPU**: Colab provides free access to Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs), which significantly accelerates the training of machine learning models.\n",
        "- **Cloud-based Environment**: Users can access and work on Colab notebooks directly through a web browser, eliminating the need for local installations. This cloud-based approach facilitates collaboration and allows users to work on projects from different devices.\n",
        "- **Pre-installed Libraries**: Colab comes pre-installed with many commonly used libraries and frameworks, such as TensorFlow, PyTorch, and scikit-learn, making it convenient for users to start working on machine learning tasks without additional setup.\n",
        "- **Integration with Google Drive**: Colab is seamlessly integrated with Google Drive, enabling users to save and share their Colab notebooks directly in their Google Drive accounts. This integration simplifies version control and sharing of projects.\n",
        "- **Real-time Collaboration**: Multiple users can collaborate on the same Colab notebook simultaneously. Changes are reflected in real-time, and users can leave comments for each other within the notebook.\n",
        "- **Version History**: Colab automatically saves version history, allowing users to revert to previous versions of their notebooks. This feature is beneficial for tracking changes and recovering from unintended modifications.\n",
        "- **Support for Markdown**: Colab supports Markdown, enabling users to add formatted text, headers, and multimedia elements to enhance the documentation within their notebooks.\n",
        "\n",
        "The following instructions need to be followed for integrating the Colab notebook with your Google Drive."
      ],
      "metadata": {
        "id": "JH97fEpCv3LJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To read files from Google drive in Colab notebook, we need to \"mount the drive\" using the command below:"
      ],
      "metadata": {
        "id": "eYKv9gQONXlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Once you mount your google drive, you can read data from your google drive into the colab notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biDHNgQxwUu9",
        "outputId": "979e23bb-79a5-4e90-b77e-1d650003b2da"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SdTbv4rgn-gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # second (alternate) way to upload files to colab\n",
        "# # use this to import files from your system to the colab environment\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "OEZLvy7Eyfdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the Google drive is mounted, you can access the folders and the files in your Google drive.\n",
        "\n",
        "To read files directtly from your Google Drive, you need to create the corresponding folder structure:\n",
        "\n",
        "Go to ```My Drive```, create a folder ```GenAI_Course_Master/Course_1_ShopAssistAI/Week_2/Session_1```\n",
        "\n",
        "The folder structure would be as shown below:\n",
        "```\n",
        "|-MyDrive\n",
        "|--GenAI_Course_Master/\n",
        "|---Course_1_ShopAssistAI/\n",
        "|----Week_2/\n",
        "|-----Session_1/\n",
        "```\n",
        "\n",
        "This is the folder where you will store this notebook and other associated files (such as API keys, data files etc).\n",
        "\n",
        "You can then access all the files in that folder at the following filepath\n",
        "\n",
        "\n",
        "filepath = ```/content/drive/MyDrive/upgrad/GenAI_Course_Master/Course_1_ShopAssistAI/Week_2/Session_1/```"
      ],
      "metadata": {
        "id": "F1RjisTEwuYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = '/content/drive/MyDrive/GenAI_Course_Master/Course_1_ShopAssistAI/Week_2/Session_1/'"
      ],
      "metadata": {
        "id": "PGscQrIUxdw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# linux command to print all files in a directory\n",
        "!ls \"/content/drive/MyDrive/GenAI_Course_Master/Course_1_ShopAssistAI/Week_2/Session_1/\""
      ],
      "metadata": {
        "id": "6eC_BgxswJTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ea6927-1f81-4a89-bb34-289429bd157b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " AI_tutor_system_message_1.txt\t        tata_motors_transcript_sample.txt\n",
            " earnings-call-transcript-q4-fy23.pdf   tata_transcript.txt\n",
            " OpenAI_API_Key.txt\t\t       'Working_With_OpenAI_APIs _ Notebook.ipynb'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import openai and set the API key\n",
        "#import openai\n",
        "\n",
        "key_path = \"/home/sola/Upgrad_couse_assignments/GenAI_course/Module2/\"\n",
        "with open(key_path + \"OpenAI_API_Key.txt\", \"r\") as f:\n",
        "\n",
        "  openai.api_key = ' '.join(f.readlines())"
      ],
      "metadata": {
        "id": "INJIL5XJzIre",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "2c135bce-97ee-4923-ecee-0beaef9eaf68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/home/sola/Upgrad_couse_assignments/GenAI_course/Module2/OpenAI_API_Key.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f03200e7a8c2>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkey_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/sola/Upgrad_couse_assignments/GenAI_course/Module2/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"OpenAI_API_Key.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sola/Upgrad_couse_assignments/GenAI_course/Module2/OpenAI_API_Key.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = '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'"
      ],
      "metadata": {
        "id": "OL7LBneMtN56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use any of the OpenAI APIs. For natural language (text) and code, for any given model (GPT-3, 3.5 or 4), OpenAI provides Chat completion capability via the `chat.completions()` endpoint. This API can be used for chat-like multi-turn conversation and takes the entire conversation history as input and returns the next response.\n",
        "\n",
        "[The official API documentation here](https://platform.openai.com/docs/guides/text-generation) explains everything in detail - We use the model `gpt-3.5-turbo`, which belongs to the GPT-3.5 family.\n",
        "\n",
        "**NOTE**: Throughout this notebook/course, we will be using the `gpt-3.5-turbo` model (it is a cheaper than its other ones in the 3.5 family, more on that later)."
      ],
      "metadata": {
        "id": "LMdzXTS8zMci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatCompletion API:\n",
        "\n",
        "Prompts to the ChatCompletion API are sent using the `messages` parameter.\n",
        "\n",
        "The `Messages` parameter must be an array of message objects, where each object has a role (either \"system\", \"user\", or \"assistant\") and content.\n",
        "\n",
        "Conversations can be as short as one message or many back and forth turns.\n",
        "\n",
        "The **`chat.completions()`** API requires three main roles to be specified in the API:\n",
        "\n",
        "1. **System**: This is an instruction that sets the behaviour of the assistant. You can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. For example, \"You are a helpful math tutor\", or \"You are a helpful advisor for financial analysts\".\n",
        "\n",
        "  However note that the system message is optional and the model's behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
        "\n",
        "2. **Assistant**: This role represents the OpenAI model.\n",
        "\n",
        "3. **User**: This role represents the end user using the chatbot.\n",
        "The user messages provide requests or comments for the assistant to respond to.\n",
        "\n",
        "Typically, a conversation is formatted with a system message first, followed by alternating user and assistant messages as below:\n",
        "```\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You're a helpful assistant},\n",
        "    {\"role\": \"user\", \"content\": user_message},\n",
        "    {\"role\": \"assistant\", \"content\": assistant_response}\n",
        "    ]\n",
        "  ```"
      ],
      "metadata": {
        "id": "1EbEHdvlGh0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of a simple ChatCompletion API call\n",
        "\n",
        "# Initialize the OpenAI API client with the provided API key\n",
        "\n",
        "# Define the input messages for the chat completion API call\n",
        "# The messages include a system message and a user message\n",
        "# System message sets the role and provides context for the assistant\n",
        "# User message contains the user's query\n",
        "\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Where was the 2015 ICC World Cup final held?\"}\n",
        "    ]\n",
        "\n",
        "# Make a chat completion API call using the GPT-3.5-turbo model\n",
        "chat_response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "# Print chat response as needed\n",
        "chat_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M-TS3o6Se7f",
        "outputId": "0a539904-706b-4861-c600-bfc2177e9b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-96IcsQnlOCXzTUuGPvR598OLwQThJ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The 2015 ICC World Cup final was held at the Melbourne Cricket Ground (MCG) in Melbourne, Australia. Australia defeated New Zealand in the final to win the tournament.', role='assistant', function_call=None, tool_calls=None))], created=1711288594, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_3bc1b5746c', usage=CompletionUsage(completion_tokens=36, prompt_tokens=29, total_tokens=65))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The type of the chat_response object\n",
        "print(type(chat_response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YPQI16mzTyt",
        "outputId": "310feeb3-e781-4eb1-faf0-1ba5fab775ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The API returns a dictionary-like object.\n",
        "\n",
        "Also, notice that the API returns the number of `total_tokens` used (prompt tokens + completion tokens).\n",
        "\n",
        "The reply we are interested in is the `text` inside `choices`, which seems to be a list. We can access that as follows:"
      ],
      "metadata": {
        "id": "9P1ZzLT6zYhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve the response text\n",
        "chat_response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "7aj7T1JWzWST",
        "outputId": "56caa2c9-949d-43ec-d912-fca5937aba77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The 2015 ICC World Cup final was held at the Melbourne Cricket Ground (MCG) in Melbourne, Australia. Australia defeated New Zealand in the final to win the tournament.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `user` input and `assistant` outputs are appended to `messages` list alternatively, which is a list that contains the entire conversation history.\n",
        "\n",
        "In each subsequent API call, we pass on the entire conversation history, which serves as memory/context for the chatbot to provide further responses."
      ],
      "metadata": {
        "id": "MdaPbiDAVRQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "eSoLdQM1RM4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ChatCompletions API can also take additional parameters. You can refer to the [API reference](https://platform.openai.com/docs/api-reference/chat/create) for the full set of parameters that the API supported by the ChatCompletions APIs. In this notebook, we will refer to some of the commmonly used paramters such as:\n",
        "\n",
        "* `max_tokens` refers to the max number of tokens to be generated.\n",
        "\n",
        "* `temperature` is a number between 0 (most certain/deterministic) and 1 (most random), defaults to 0.\n",
        "\n",
        "* `n` the number of chat completion choices to generate for each input message\n",
        "\n",
        "* `stop` Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n",
        "\n",
        "* `frequency_penalty` and `presence_penalty` are used to reduce the likelihood of sampling repetitive sequences of tokens. The recommended values for the penalty coefficients are around 0.1 to 1 if the aim is to just reduce repetitive tokens in the output response. If the aim is to strongly suppress repetition, then one can increase the coefficients up to 2, but this comes at a decreased quality of samples.\n",
        "\n"
      ],
      "metadata": {
        "id": "a8NEU8qVWxoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an input prompt\n",
        "prompt = '''You are a helpful Python teaching assistant.\n",
        "Explain the various list indexing methods in Python.\n",
        "Provide an exhaustive summary of the methods describing what they do,\n",
        "sample code for each, and guidelines on when to use which method.\n",
        "'''\n",
        "\n",
        "# Notice there's no system message\n",
        "message = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "\n",
        "chat_response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=message,\n",
        "    max_tokens=200,\n",
        "    temperature=0.5,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0.5)\n",
        "\n",
        "chat_response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "4RTaTK5mzMvk",
        "outputId": "0d35ed05-59d4-4aa1-d15f-6bd36204cd35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'List indexing in Python allows you to access individual elements within a list by their position. There are various methods for indexing lists in Python, including positive indexing, negative indexing, slicing, and using the `index()` method.\\n\\n1. Positive Indexing:\\nPositive indexing starts at 0 for the first element in the list and increases by 1 for each subsequent element. To access an element at a specific index, simply use the index number enclosed in square brackets.\\n\\nSample code:\\n```python\\nmy_list = [10, 20, 30, 40, 50]\\nprint(my_list[2])  # Output: 30\\n```\\n\\nUse positive indexing when you know the exact position of the element you want to access.\\n\\n2. Negative Indexing:\\nNegative indexing starts at -1 for the last element in the list and decreases by 1 for each preceding element. To access an element using negative indexing, use a negative index number enclosed in square brackets.\\n\\nSample code:\\n```python'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSexfPUZKAqJ",
        "outputId": "a62c73c8-2e2e-445e-efea-726a6b3b82cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List indexing in Python allows you to access individual elements within a list by their position. There are various methods for indexing lists in Python, including positive indexing, negative indexing, slicing, and using the `index()` method.\n",
            "\n",
            "1. Positive Indexing:\n",
            "Positive indexing starts at 0 for the first element in the list and increases by 1 for each subsequent element. To access an element at a specific index, simply use the index number enclosed in square brackets.\n",
            "\n",
            "Sample code:\n",
            "```python\n",
            "my_list = [10, 20, 30, 40, 50]\n",
            "print(my_list[2])  # Output: 30\n",
            "```\n",
            "\n",
            "Use positive indexing when you know the exact position of the element you want to access.\n",
            "\n",
            "2. Negative Indexing:\n",
            "Negative indexing starts at -1 for the last element in the list and decreases by 1 for each preceding element. To access an element using negative indexing, use a negative index number enclosed in square brackets.\n",
            "\n",
            "Sample code:\n",
            "```python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can increase the number of `max_tokens` to get a more detailed response."
      ],
      "metadata": {
        "id": "yweiksDjEnAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# increased number of tokens\n",
        "chat_response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages = message,\n",
        "    max_tokens=800,\n",
        "    temperature=0.5,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0)\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fCzVUz1zdY9",
        "outputId": "956d2c29-2755-4996-ab23-31844f9a1a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Python, lists are ordered collections of items that can be accessed using indexing. There are several methods for indexing lists in Python, each serving a specific purpose. Here is an exhaustive summary of the list indexing methods along with sample code and guidelines on when to use each method:\n",
            "\n",
            "1. Positive Indexing:\n",
            "Positive indexing starts from 0 for the first element in the list and increments by 1 for each subsequent element.\n",
            "\n",
            "Sample code:\n",
            "```python\n",
            "my_list = [10, 20, 30, 40, 50]\n",
            "print(my_list[0])  # Output: 10\n",
            "print(my_list[2])  # Output: 30\n",
            "```\n",
            "\n",
            "Use positive indexing when you want to access elements from the beginning of the list.\n",
            "\n",
            "2. Negative Indexing:\n",
            "Negative indexing starts from -1 for the last element in the list and decrements by 1 for each preceding element.\n",
            "\n",
            "Sample code:\n",
            "```python\n",
            "my_list = [10, 20, 30, 40, 50]\n",
            "print(my_list[-1])  # Output: 50\n",
            "print(my_list[-3])  # Output: 30\n",
            "```\n",
            "\n",
            "Use negative indexing when you want to access elements from the end of the list.\n",
            "\n",
            "3. Slicing:\n",
            "Slicing allows you to access a subset of elements in a list by specifying a start index, end index, and step size.\n",
            "\n",
            "Sample code:\n",
            "```python\n",
            "my_list = [10, 20, 30, 40, 50]\n",
            "print(my_list[1:4])  # Output: [20, 30, 40]\n",
            "print(my_list[::2])  # Output: [10, 30, 50]\n",
            "```\n",
            "\n",
            "Use slicing when you want to extract a portion of the list.\n",
            "\n",
            "4. Extended Slicing:\n",
            "Extended slicing allows you to specify a start index, end index, and step size for slicing, along with the ability to reverse the list.\n",
            "\n",
            "Sample code:\n",
            "```python\n",
            "my_list = [10, 20, 30, 40, 50]\n",
            "print(my_list[::-1])  # Output: [50, 40, 30, 20, 10]\n",
            "print(my_list[4:1:-1])  # Output: [50, 40, 30]\n",
            "```\n",
            "\n",
            "Use extended slicing when you want to reverse a list or extract a portion of the list in a reversed order.\n",
            "\n",
            "These are the various list indexing methods in Python along with guidelines on when to use each method. Understanding and mastering these methods will help you efficiently work with lists in Python.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `response_format` lets you control the output format of the model's response. This parameter is compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.\n",
        "\n",
        "Setting `response_format` to { \"type\": \"json_object\" } enables JSON mode, which guarantees the message the model generates is valid JSON. JSON stands for Java Script Object Notation - which is a standard format in which the response analogous to nested dictionary. It is easy for humans to read and write and easy for machines to parse and generate. JSON is often used to transmit data between a server and a web application, as well as for configuration files.\n",
        "\n",
        "JSON data is represented as key-value pairs in a simple text format. Here's a basic example:\n",
        "```\n",
        "{\n",
        "  \"name\": \"John Doe\",\n",
        "  \"age\": 30,\n",
        "  \"city\": \"New York\",\n",
        "  \"isStudent\": false,\n",
        "  \"courses\": [\"Math\", \"History\", \"Computer Science\"]\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "**NOTE**: when using JSON mode, you must also instruct the model to produce JSON via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request.\n"
      ],
      "metadata": {
        "id": "djHMj_NWZ_1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a request to the GPT-3.5 Turbo model for chat-based completion\n",
        "chat_response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",  # Specify the GPT-3.5 Turbo model\n",
        "    response_format={ \"type\": \"json_object\" },  # Request the response in JSON format\n",
        "    messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON to the key 'answer'.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the ICC World Cup in 2015?\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "# Extract the output from the response\n",
        "output = chat_response.choices[0].message.content\n",
        "\n",
        "# Print the generated output\n",
        "print(output)\n",
        "\n",
        "# Print the type of the output (usually a string)\n",
        "print(\"Output Type:\", type(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgPqgQaed-Wn",
        "outputId": "59357780-ad92-4d07-b1a0-1a347b50b132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"answer\": \"Australia\"\n",
            "}\n",
            "Output Type: <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the json module to work with JSON data in Python.\n",
        "import json\n",
        "\n",
        "# Assuming 'output' is a string containing JSON-formatted data, use json.loads() to convert it into a Python dictionary.\n",
        "json_output = json.loads(output)\n",
        "\n",
        "# Use the get() method to retrieve the value associated with the key 'answer' from the JSON data.\n",
        "# If the 'answer' key is not present in the JSON data, return the default value \"None\".\n",
        "result = json_output.get('answer', \"None\")\n",
        "\n",
        "# The 'result' variable now contains the value associated with the 'answer' key or \"None\" if the key is not present.\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amsFVzVxdzGk",
        "outputId": "824fc6a0-4c4d-4b04-a237-fd88083b0605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Australia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating More Complex Prompts\n",
        "We can now modify the prompts such that users can provide inputs to it. For e.g. we may want to let the user specify the name of a topic."
      ],
      "metadata": {
        "id": "OQus9GhYNyYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_prompt = '''You are a helpful Python teaching assistant.\n",
        "Explain the following topic in detail.\n",
        "Provide an exhaustive summary of the methods describing what they do,\n",
        "sample code for each, and guidelines on when to use which method.\n",
        "The topic is: {0}'''\n",
        "\n",
        "# topic as input\n",
        "topic_name = \"Indexing in Pandas Dataframes\"\n",
        "\n",
        "prompt1 = base_prompt.format(topic_name)\n",
        "print(prompt1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGJj_d9D8Iuu",
        "outputId": "a4aceca3-1b97-474b-b111-7f3cce59abeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a helpful Python teaching assistant.\n",
            "Explain the following topic in detail.\n",
            "Provide an exhaustive summary of the methods describing what they do,\n",
            "sample code for each, and guidelines on when to use which method.\n",
            "The topic is: Indexing in Pandas Dataframes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [{\"role\": \"user\", \"content\": prompt1}]\n",
        "\n",
        "# call the API with the new prompt\n",
        "chat_response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=message,\n",
        "    max_tokens=800,\n",
        "    temperature=0.5,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0)\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "eyK9eYOLNp-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5601c5c-df6c-47ef-c1dd-ef8358cfd5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing in Pandas Dataframes:\n",
            "Indexing in Pandas Dataframes allows you to access and manipulate data within the dataframe. There are several methods available for indexing in Pandas Dataframes, each serving a unique purpose.\n",
            "\n",
            "1. iloc[]: This method is used for integer-based indexing. It allows you to access data by specifying the row and column numbers. The syntax for iloc[] is dataframe.iloc[row_index, column_index].\n",
            "\n",
            "Sample code:\n",
            "```python\n",
            "import pandas as pd\n",
            "\n",
            "data = {'A': [1, 2, 3, 4],\n",
            "        'B': [5, 6, 7, 8]}\n",
            "\n",
            "df = pd.DataFrame(data)\n",
            "\n",
            "# Accessing data using iloc[]\n",
            "print(df.iloc[0, 1])  # Output: 5\n",
            "```\n",
            "\n",
            "Use iloc[] when you want to access data based on integer position.\n",
            "\n",
            "2. loc[]: This method is used for label-based indexing. It allows you to access data by specifying the row and column labels. The syntax for loc[] is dataframe.loc[row_label, column_label].\n",
            "\n",
            "Sample code:\n",
            "```python\n",
            "import pandas as pd\n",
            "\n",
            "data = {'A': [1, 2, 3, 4],\n",
            "        'B': [5, 6, 7, 8]}\n",
            "\n",
            "df = pd.DataFrame(data, index=['row1', 'row2', 'row3', 'row4'])\n",
            "\n",
            "# Accessing data using loc[]\n",
            "print(df.loc['row2', 'B'])  # Output: 6\n",
            "```\n",
            "\n",
            "Use loc[] when you want to access data based on labels.\n",
            "\n",
            "3. at[]: This method is similar to loc[], but is faster for accessing a single value. It is used for label-based indexing. The syntax for at[] is dataframe.at[row_label, column_label].\n",
            "\n",
            "Sample code:\n",
            "```python\n",
            "import pandas as pd\n",
            "\n",
            "data = {'A': [1, 2, 3, 4],\n",
            "        'B': [5, 6, 7, 8]}\n",
            "\n",
            "df = pd.DataFrame(data, index=['row1', 'row2', 'row3', 'row4'])\n",
            "\n",
            "# Accessing data using at[]\n",
            "print(df.at['row2', 'B'])  # Output: 6\n",
            "```\n",
            "\n",
            "Use at[] when you want to access a single value quickly based on labels.\n",
            "\n",
            "4. iat[]: This method is similar to iloc[], but is faster for accessing a single value. It is used for integer-based indexing. The syntax for iat[] is dataframe.iat[row_index, column_index].\n",
            "\n",
            "Sample code:\n",
            "```python\n",
            "import pandas as pd\n",
            "\n",
            "data = {'A': [1, 2, 3, 4],\n",
            "        'B': [5, 6, 7, 8]}\n",
            "\n",
            "df = pd.DataFrame(data)\n",
            "\n",
            "# Accessing data using iat[]\n",
            "print(df.iat[0, 1])  # Output: 5\n",
            "```\n",
            "\n",
            "Use iat[] when you want to access a single value quickly based on integer position.\n",
            "\n",
            "In summary, use iloc[] for integer-based indexing, loc[] for label-based indexing, at[] for fast label-based single value access, and iat[] for fast integer-based single value access. Choose the appropriate method based on the type of indexing and the specific requirements of your data manipulation tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# topic as input\n",
        "topic_name = \"Lambdas & functions in Python\"\n",
        "prompt2 = base_prompt.format(topic_name)\n",
        "print(prompt2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrS3vm0vN_hL",
        "outputId": "ed49297c-0923-4082-df6e-e6681a86222a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a helpful Python teaching assistant.\n",
            "Explain the following topic in detail.\n",
            "Provide an exhaustive summary of the methods describing what they do,\n",
            "sample code for each, and guidelines on when to use which method.\n",
            "The topic is: Lambdas & functions in Python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the application, we often want to provide additional instructions to the prompt, such as *explain at a beginner level*, *explain step by step*, or any other specific, detailed information, such as *use the following two page document to answer the user's question*. We can do that by some simple text manipulation hacks.\n",
        "\n",
        "<br>\n",
        "\n",
        "For example, say you want to develop an information retrieval app for Financial Analysts which can provide them information from documents such as investor presentations, annual reports, quarterly earnings calls, etc. As a sample, see this earnings call of an Indian automobile company, Tata Motors.\n",
        "\n",
        "For demonstration, we have taken a small toy-sized sample of this transcript and put it in a txt file `tata_motors_transcript_sample.txt` (this file is uploaded along with the colab notebook, so we can read the file directly as shown below).\n",
        "\n",
        "If you want to use any other file that you have on your system, you can upload it via the following command:"
      ],
      "metadata": {
        "id": "DN8SCbGKQDMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use this to import files from your system to the colab environment\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "_V96FYQgIMjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(filepath + \"tata_motors_transcript_sample.txt\", \"r\") as f:\n",
        "  transcript = ' '.join(f.readlines())\n",
        "\n",
        "print(len(transcript))\n",
        "print(transcript)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrO5KxOrafnU",
        "outputId": "638633bf-91b2-4abe-b592-5cac63f59710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3370\n",
            "Fair to say it  has been an extremely satisfying quarter. And the reason I say, use  that word is that, nice to \n",
            "  see all the auto verticals coming together once again and this time  with a lot of intensity as well. So both \n",
            "  the alignment of the vectors are there and the magnitude  of vectors are also increasing, which is what has \n",
            "  translated into a strong set of numbers  for the quarters, resulting on multiple highs  and I will quickly  cover \n",
            "  that in the coming slides . We en ded the year on a pretty strong note with revenue of  around  Rs. 1 lakh \n",
            "  crores  with an EBITDA of 13.3% , and the profit before tax and exceptional item of Rs. 5,000  crores.  \n",
            "  On a full -year basis, we hit our highest ever revenue at  around  Rs. 3.5 lakh crores and ended the year  with \n",
            "  a positive free cash flow  of Rs. 7,800 crores, despite a very weak start in Q1 and  Q2, which you see in the \n",
            "  numbers . The business has been sequentially improving its performance  and doing it in significant \n",
            "  stretches. So, very happy with the way we have ended . Next slide please.  \n",
            "  Source of growt h, in the quarter we grew by about 35% , of which  volume  and mix contributed 24% of the \n",
            "  35% and the price came in at 10.5%, so price is still  a very strong variable in our growth agenda. \n",
            "  Profitability 3.2% went to 6.8% and all businesses  contributing to it and JLR coming at a very large swing \n",
            "  there, contributing to the  2.8% out of that delta there. Again debt continues to reduce at Rs. 43,700 crores \n",
            "  and TML India at Rs. 6,200 crores, JLR GBP  3 billion i.e. Rs. 30,000 crores we talk about. So  the FY 24 net \n",
            "  debt reduction plan, we are confirming, we will not be in a position to  meet the FY 24 numbers, but very \n",
            "  clearly  - FY25 we want this issue sorted out.  Next slide.  \n",
            "  A series of headlines, normally we don't do that, but it's fair to just pull back  and reflect  on what this \n",
            "  quarter and the year actually means for the Group . For us , each one of us in this call, we feel strongly \n",
            "  about this, the highest -ever revenue in a  quarter, the highest -ever EBITDA in a quarter, one of the \n",
            "  strongest PBTs that we  delivered and all three auto vertical simultaneously profitable, strong net debt \n",
            "  reduction.  That's all the quarter numbers. And if you factor in  that the first half was a very weak first half,  \n",
            "  we still delivered the highest -ever revenue. We have the highest EBITDA since  2015, very  strong PBT \n",
            "  despite the weak start , after factoring the weak start. And India net debt  lowest in the last 15 years, so \n",
            "  it's worthwhile just to  mull over  it to say, I would say, that the  potential of this business is slowly starting \n",
            "  to come out and that's what we would want to  build on when we go into FY 24. This is basically giving us \n",
            "  the impetus to how we want to  play at FY 24. Next slide.  \n",
            "  This is also something which we are happy about, a dividend of Rs. 2 per share for  the ordinary \n",
            "  shareholder s and Rs. 2.1 for the DVR shareholders, coming in after sometime. And this will obviously have \n",
            "  to be a - the Board has recommended this and this will have to be approved  in the ensuing shareholders \n",
            "  meeting. And this will result in a cash flow of  Rs. 771 crores as part of the plan i.e. this  is factored into the \n",
            "  debt reduction plan in any case.  So again, very happy to see this.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we design a prompt comprising of three entities - a) the base instruction which specifies the task to ChatGPT, b) the question asked by the user (analyst), c) the earnings call transcript using which it is supposed to find an answer."
      ],
      "metadata": {
        "id": "2_hIEDSDIcLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_instruction = '''You are a helpful assistant which helps financial analysts retrieve\n",
        "relevant financial and business related information from documents.\n",
        "Given below is a question and the transcript of an earnings call of an automobile company, Tata Motors,\n",
        "which was attended by the top management of the firm.\n",
        "Try to respond with specific numbers and facts wherever possible.\n",
        "If you are not sure about the accuracy of the information, just respond that you do not know'''\n",
        "\n",
        "question = \"How much free cash flow did Tata Motors have at the end of the year?\"\n",
        "\n",
        "prompt = base_instruction + \"\\n\\n\" +  \"Question: {0}\".format(question) + \"\\n\\n\" + \"Transcript: \\n {0}\".format(transcript)\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNDGlBEy8WAY",
        "outputId": "1b78e8cf-d3de-41e2-c199-701712f5e70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a helpful assistant which helps financial analysts retrieve\n",
            "relevant financial and business related information from documents.\n",
            "Given below is a question and the transcript of an earnings call of an automobile company, Tata Motors,\n",
            "which was attended by the top management of the firm.\n",
            "Try to respond with specific numbers and facts wherever possible.\n",
            "If you are not sure about the accuracy of the information, just respond that you do not know\n",
            "\n",
            "Question: How much free cash flow did Tata Motors have at the end of the year?\n",
            "\n",
            "Transcript: \n",
            " Fair to say it  has been an extremely satisfying quarter. And the reason I say, use  that word is that, nice to \n",
            "  see all the auto verticals coming together once again and this time  with a lot of intensity as well. So both \n",
            "  the alignment of the vectors are there and the magnitude  of vectors are also increasing, which is what has \n",
            "  translated into a strong set of numbers  for the quarters, resulting on multiple highs  and I will quickly  cover \n",
            "  that in the coming slides . We en ded the year on a pretty strong note with revenue of  around  Rs. 1 lakh \n",
            "  crores  with an EBITDA of 13.3% , and the profit before tax and exceptional item of Rs. 5,000  crores.  \n",
            "  On a full -year basis, we hit our highest ever revenue at  around  Rs. 3.5 lakh crores and ended the year  with \n",
            "  a positive free cash flow  of Rs. 7,800 crores, despite a very weak start in Q1 and  Q2, which you see in the \n",
            "  numbers . The business has been sequentially improving its performance  and doing it in significant \n",
            "  stretches. So, very happy with the way we have ended . Next slide please.  \n",
            "  Source of growt h, in the quarter we grew by about 35% , of which  volume  and mix contributed 24% of the \n",
            "  35% and the price came in at 10.5%, so price is still  a very strong variable in our growth agenda. \n",
            "  Profitability 3.2% went to 6.8% and all businesses  contributing to it and JLR coming at a very large swing \n",
            "  there, contributing to the  2.8% out of that delta there. Again debt continues to reduce at Rs. 43,700 crores \n",
            "  and TML India at Rs. 6,200 crores, JLR GBP  3 billion i.e. Rs. 30,000 crores we talk about. So  the FY 24 net \n",
            "  debt reduction plan, we are confirming, we will not be in a position to  meet the FY 24 numbers, but very \n",
            "  clearly  - FY25 we want this issue sorted out.  Next slide.  \n",
            "  A series of headlines, normally we don't do that, but it's fair to just pull back  and reflect  on what this \n",
            "  quarter and the year actually means for the Group . For us , each one of us in this call, we feel strongly \n",
            "  about this, the highest -ever revenue in a  quarter, the highest -ever EBITDA in a quarter, one of the \n",
            "  strongest PBTs that we  delivered and all three auto vertical simultaneously profitable, strong net debt \n",
            "  reduction.  That's all the quarter numbers. And if you factor in  that the first half was a very weak first half,  \n",
            "  we still delivered the highest -ever revenue. We have the highest EBITDA since  2015, very  strong PBT \n",
            "  despite the weak start , after factoring the weak start. And India net debt  lowest in the last 15 years, so \n",
            "  it's worthwhile just to  mull over  it to say, I would say, that the  potential of this business is slowly starting \n",
            "  to come out and that's what we would want to  build on when we go into FY 24. This is basically giving us \n",
            "  the impetus to how we want to  play at FY 24. Next slide.  \n",
            "  This is also something which we are happy about, a dividend of Rs. 2 per share for  the ordinary \n",
            "  shareholder s and Rs. 2.1 for the DVR shareholders, coming in after sometime. And this will obviously have \n",
            "  to be a - the Board has recommended this and this will have to be approved  in the ensuing shareholders \n",
            "  meeting. And this will result in a cash flow of  Rs. 771 crores as part of the plan i.e. this  is factored into the \n",
            "  debt reduction plan in any case.  So again, very happy to see this.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZopGTZfAfIg",
        "outputId": "8af8a577-09de-4989-8018-d7b7af7c21ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user',\n",
              "  'content': \"You are a helpful assistant which helps financial analysts retrieve\\nrelevant financial and business related information from documents.\\nGiven below is a question and the transcript of an earnings call of an automobile company, Tata Motors,\\nwhich was attended by the top management of the firm.\\nTry to respond with specific numbers and facts wherever possible.\\nIf you are not sure about the accuracy of the information, just respond that you do not know\\n\\nQuestion: How much free cash flow did Tata Motors have at the end of the year?\\n\\nTranscript: \\n Fair to say it  has been an extremely satisfying quarter. And the reason I say, use  that word is that, nice to \\n  see all the auto verticals coming together once again and this time  with a lot of intensity as well. So both \\n  the alignment of the vectors are there and the magnitude  of vectors are also increasing, which is what has \\n  translated into a strong set of numbers  for the quarters, resulting on multiple highs  and I will quickly  cover \\n  that in the coming slides . We en ded the year on a pretty strong note with revenue of  around  Rs. 1 lakh \\n  crores  with an EBITDA of 13.3% , and the profit before tax and exceptional item of Rs. 5,000  crores.  \\n  On a full -year basis, we hit our highest ever revenue at  around  Rs. 3.5 lakh crores and ended the year  with \\n  a positive free cash flow  of Rs. 7,800 crores, despite a very weak start in Q1 and  Q2, which you see in the \\n  numbers . The business has been sequentially improving its performance  and doing it in significant \\n  stretches. So, very happy with the way we have ended . Next slide please.  \\n  Source of growt h, in the quarter we grew by about 35% , of which  volume  and mix contributed 24% of the \\n  35% and the price came in at 10.5%, so price is still  a very strong variable in our growth agenda. \\n  Profitability 3.2% went to 6.8% and all businesses  contributing to it and JLR coming at a very large swing \\n  there, contributing to the  2.8% out of that delta there. Again debt continues to reduce at Rs. 43,700 crores \\n  and TML India at Rs. 6,200 crores, JLR GBP  3 billion i.e. Rs. 30,000 crores we talk about. So  the FY 24 net \\n  debt reduction plan, we are confirming, we will not be in a position to  meet the FY 24 numbers, but very \\n  clearly  - FY25 we want this issue sorted out.  Next slide.  \\n  A series of headlines, normally we don't do that, but it's fair to just pull back  and reflect  on what this \\n  quarter and the year actually means for the Group . For us , each one of us in this call, we feel strongly \\n  about this, the highest -ever revenue in a  quarter, the highest -ever EBITDA in a quarter, one of the \\n  strongest PBTs that we  delivered and all three auto vertical simultaneously profitable, strong net debt \\n  reduction.  That's all the quarter numbers. And if you factor in  that the first half was a very weak first half,  \\n  we still delivered the highest -ever revenue. We have the highest EBITDA since  2015, very  strong PBT \\n  despite the weak start , after factoring the weak start. And India net debt  lowest in the last 15 years, so \\n  it's worthwhile just to  mull over  it to say, I would say, that the  potential of this business is slowly starting \\n  to come out and that's what we would want to  build on when we go into FY 24. This is basically giving us \\n  the impetus to how we want to  play at FY 24. Next slide.  \\n  This is also something which we are happy about, a dividend of Rs. 2 per share for  the ordinary \\n  shareholder s and Rs. 2.1 for the DVR shareholders, coming in after sometime. And this will obviously have \\n  to be a - the Board has recommended this and this will have to be approved  in the ensuing shareholders \\n  meeting. And this will result in a cash flow of  Rs. 771 crores as part of the plan i.e. this  is factored into the \\n  debt reduction plan in any case.  So again, very happy to see this.\\n\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# call the API with the new prompt\n",
        "chat_response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=message,\n",
        "    max_tokens=1000,\n",
        "    temperature=0.5,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0)\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fslRdgwh-kwR",
        "outputId": "588c7c4f-af55-42e2-a5fb-fd07cdbae660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At the end of the year, Tata Motors had a positive free cash flow of Rs. 7,800 crores.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# another question\n",
        "question = \"Summarise the key financial metrics reported in the earnings call related to revenue growth, profitability, cash flow and debt.\"\n",
        "\n",
        "prompt = base_instruction + \"\\n\\n\" +  \"Question: {0}\".format(question) + \"\\n\\n\" + \"Transcript: \\n {0}\".format(transcript)\n",
        "\n",
        "message = [{\"role\":\"user\",\"content\":prompt}]\n",
        "\n",
        "chat_response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=message,\n",
        "    max_tokens=1000,\n",
        "    temperature=0.5,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0)\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE52r251-wqX",
        "outputId": "86286dc9-4cff-4094-c867-b69359f58908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key financial metrics reported in the earnings call related to revenue growth, profitability, cash flow, and debt are as follows:\n",
            "\n",
            "1. Revenue:\n",
            "- Ended the year with revenue of around Rs. 1 lakh crores for the quarter.\n",
            "- Hit highest ever revenue at around Rs. 3.5 lakh crores for the full year.\n",
            "\n",
            "2. Profitability:\n",
            "- EBITDA was reported at 13.3% for the quarter.\n",
            "- Profit before tax and exceptional items was Rs. 5,000 crores for the quarter.\n",
            "- Profitability increased from 3.2% to 6.8% for the quarter, with all businesses contributing to it.\n",
            "\n",
            "3. Cash Flow:\n",
            "- Ended the year with a positive free cash flow of Rs. 7,800 crores.\n",
            "- Dividend of Rs. 2 per share for ordinary shareholders and Rs. 2.1 for DVR shareholders, resulting in a cash flow of Rs. 771 crores.\n",
            "\n",
            "4. Debt:\n",
            "- Debt continues to reduce, with total debt at Rs. 43,700 crores, TML India at Rs. 6,200 crores, and JLR GBP 3 billion (Rs. 30,000 crores).\n",
            "- FY 24 net debt reduction plan may not be met, but the aim is to resolve the issue by FY25.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ask something not mentioned in the transcript sample\n",
        "question = \"How much equity funding did Tata Motors raise from institutional investors in this quarter?\"\n",
        "\n",
        "prompt = base_instruction + \"\\n\\n\" +  \"Question: {0}\".format(question) + \"\\n\\n\" + \"Transcript: \\n {0}\".format(transcript)\n",
        "\n",
        "message = [{\"role\":\"user\",\"content\":prompt}]\n",
        "\n",
        "chat_response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=message,\n",
        "    max_tokens=1000,\n",
        "    temperature=0.5,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0)\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UntEK50dJols",
        "outputId": "2d69759d-9d3b-49bd-db27-2527cac8f75a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I do not have the specific information on how much equity funding Tata Motors raised from institutional investors in this quarter based on the provided transcript.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### Exercise: Refining the Prompt\n",
        "* Modify the prompt so ChatGPT can do the following tasks:\n",
        " - When the user asks for information not present in the transcript, ChatGPT must respond accordingly, rather than providing an incorrect answer\n",
        " - If ChatGPT is unsure of the answer, it must provide an appropriate response"
      ],
      "metadata": {
        "id": "t-ORuzjjaPfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "In this section of the tutorial, we used the `Completion()` API to access ChatGPT via Python, wrote prompts that can take variable inputs, and tinkered with some ways to add external information to the prompts.\n",
        "\n",
        "In the next section, we will build longer, chat-like programs using the `ChatCompletion()` API."
      ],
      "metadata": {
        "id": "PYyBU1zqZ5I7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far in this notebook, we performed the following tasks:\n",
        "1. Make API calls to the `chat.completions()` endpoint\n",
        "2. Modify the prompts and make them more nuanced to perform complex tasks\n",
        "\n",
        "In the second part of working with ChatGPT APIs, we will use the `chat.completions()` API endpoint to create a simple AI-tutor. Specifically, we will:\n",
        "3. Create a simple 'AI Tutor' using the `chat.completions()` endpoint which assists students with math problems\n",
        "4. Measure the cost of making API calls via tokens and put guardrails in place to monitor and control costs"
      ],
      "metadata": {
        "id": "QHPdKqxijxW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Turn Conversation using `chat.completions()` | Math AI Tutor\n",
        "\n",
        "In the previous section, we used the `chat.completions()` endpoint to get responses for various prompts. We'll use the same endpoint to create a sample application.\n",
        "\n",
        "`chat.completions()` is used for multi-turn, chat-like conversations. For e.g., say we want to build an AI tutor application which helps students with math homework problems. Let's first define what a good tutor looks like. A good tutor will:\n",
        "\n",
        "* Not reveal the answer to the student, but rather help the student identify their mistakes by asking questions (probing)\n",
        "* Provide hints, fill gaps in the student's knowledge required to solve the problem\n",
        "* Provide feedback to guide the student if they are thinking in the right direction\n",
        "\n",
        "And this will require a conversation with multiple turns, not a single input-output transaction. This is where `chat.completions()` API is useful - the bot can be **contextually aware** and make **long, coherent conversations**.\n",
        "\n",
        "<hr>\n",
        "\n",
        "An example (good) conversation may look like this:\n",
        "* Student: Help me solve the equation x^2 - 5x + 6 = 0\n",
        "* Tutor: Sure. Which step of the solution have you reached?\n",
        "* Student: Can you tell me the answer first?\n",
        "* Tutor: As a tutor, I can help you solve the problem by providing guidance, hints or feedback. But I cannot reveal the answer since it will jeopardize your learning.\n",
        "* Student: Okay. What should be my first step to solve this equation?\n",
        "* Tutor: Try to factorize the equation, i.e. break it down in the form (x - a)(x - b) = 0.\n"
      ],
      "metadata": {
        "id": "OPNxny4TbY3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simple API call\n",
        "chat_response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI tutor that assists school students with math homework problems.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Help me solve the equation 3x - 9 = 21.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Try moving the 9 to the right hand side of the equation. What do you get?\"},\n",
        "        {\"role\": \"user\", \"content\": \"3x = 12\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# extract only the text\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWeH5bHPbaDE",
        "outputId": "5288a851-efa1-4c33-a979-53e162bef98e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now, to solve for x, divide both sides of the equation by 3. What is the value of x?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now set up some more complex initial system messages, provide more example conversations, store the progressive responses and pass them on to the API for successive conversations."
      ],
      "metadata": {
        "id": "CAbzAlBorar8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(filepath + \"AI_tutor_system_message_1.txt\", \"r\") as f:\n",
        "  system_message = ' '.join(f.readlines())\n",
        "\n",
        "print(system_message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg-su8buzduX",
        "outputId": "cad4c5fb-c590-4f37-cd3d-a6942cb5ab3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an AI tutor that assists school students with math homework problems. You never reveal the right answer to the student. You ask probing questions to identify where the student might be needing help, provide hints and guidance, and provide directional feedback to indicate if the student is moving in the right direction.\n",
            " \n",
            " Do not reveal the correct answer to the student.\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should note that the `system_message` has its limitations - while it sets the behavior of the bot to an extent, it may not completely determine the behavior. To solve for this, we can provide some examples to the bot - this is called **few shot prompting**.\n",
        "\n",
        "### Few-Shot Prompting | Providing Examples\n",
        "Few-shot prompting is the technique of providing examples of behaviours that we expect from the bot. Let's create a list with certain examples which acts as the `messages` object."
      ],
      "metadata": {
        "id": "gWCFSYxd0AH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of system message, user and assistant samples\n",
        "message_history = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": \"Help me solve the equation 3x - 9 = 21.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Sure! Try moving the 9 to the right hand side of the equation. What do you get?\"},\n",
        "        {\"role\": \"user\", \"content\": \"3x = 12\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Well, there seems to be a mistake. When you move 9 to the right hand side, you need to change its sign. Can you try again?\"},\n",
        "        {\"role\": \"user\", \"content\": \"3x = 30\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"That looks good, great job! Now, try to divide both sides by 3. What do you get?\"},\n",
        "        {\"role\": \"user\", \"content\": \"x = 10\"},\n",
        "    ]"
      ],
      "metadata": {
        "id": "OWa1Fq4m0TTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the chatbot's next response\n",
        "chat_response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages = message_history)\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9Q36IW71gwH",
        "outputId": "e1649bb9-6c9f-443e-e9de-cc5bf970e01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Awesome work! You have successfully solved for \\( x \\)! Keep practicing to strengthen your skills. If you have any more questions or need further assistance, feel free to ask!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now build a mini-program where we can input the message as an actual user / student and test our AI tutor. But notice that there is one small problem - the examples we have provided are **examples**, not actual conversations that the chatbot should refer to. To clarify that, we can specify the key `name`to `example_user` and `example_assistant`."
      ],
      "metadata": {
        "id": "EraP5lfx3AyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of system message, user and assistant examples\n",
        "message_history = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Help me solve the equation 3x - 9 = 21.\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"Sure! Try moving the 9 to the right hand side of the equation. What do you get?\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"3x = 12\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"Well, there seems to be a mistake. When you move 9 to the right hand side, you need to change its sign. Can you try again?\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"3x = 30\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"That looks good, great job! Now, try to divide both sides by 3. What do you get?\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"x = 10\"},\n",
        "        {\"role\": \"user\", \"content\": \"Help me solve the equation x - 10 = 2x\"}\n",
        "    ]"
      ],
      "metadata": {
        "id": "8o9SB6sH2pMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the chatbot's next response\n",
        "chat_response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages = message_history)\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Dg5QPt44_qm",
        "outputId": "9e7e9076-9eff-4b58-b46a-741fde176136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To solve this equation, let's start by trying to isolate x on one side of the equation. \n",
            "\n",
            "What could you do first to help isolate x?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now build a mini program which can take inputs so we can chat with our AI tutor and test it. The system message and some initial examples  are provided in `message_history` already.\n",
        "\n",
        "Let's  write a program which starts with the initial examples, can run a conversation of length n (we need to stop the program somewhere!), add a field which can take the user's input and append it to the `message_history`."
      ],
      "metadata": {
        "id": "UGPTAZMw5Mj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input()\n",
        "print(inp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoExKNaBFBQ6",
        "outputId": "2dfe9842-3f29-4ab2-9c4c-0ab4d8d9ec06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n",
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AI tutor mini program\n",
        "# Enter \"exit\" to terminate the program\n",
        "\n",
        "# One user message + one assistant message is one converation\n",
        "max_conversations = 20\n",
        "conversation_length = 0 # initialize\n",
        "\n",
        "# initialize system message, user and assistant examples\n",
        "message_history = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Help me solve the equation 3x - 9 = 21.\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"Sure! Try moving the 9 to the right hand side of the equation. What do you get?\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"3x = 12\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"Well, there seems to be a mistake. When you move 9 to the right hand side, you need to change its sign. Can you try again?\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"3x = 30\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"That looks good, great job! Now, try to divide both sides by 3. What do you get?\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"x = 10\"}]\n",
        "\n",
        "while conversation_length < max_conversations:\n",
        "  user_input = input()\n",
        "\n",
        "  # exit if user enters exit\n",
        "  if \"exit\" in user_input.lower():\n",
        "    print(\"AI Tutor: Exiting the program!\")\n",
        "    break\n",
        "\n",
        "  message_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "  chat_response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages = message_history).choices[0].message.content\n",
        "\n",
        "  print(\"\\n\", \"AI Tutor:\")\n",
        "  print(chat_response)\n",
        "  print(\"\\n\")\n",
        "\n",
        "  message_history.append({\"role\": \"assistant\", \"content\": chat_response}) # add API response to message history\n",
        "  conversation_length = conversation_length + 1 # another conversation done\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zhx0XCjx5BN9",
        "outputId": "96c20187-3b84-4692-c120-dcbaed406d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "help me solve 3x - 14 = 7\n",
            "\n",
            " AI Tutor:\n",
            "To solve the equation 3x - 14 = 7, you'll first want to isolate the variable x. Try adding 14 to both sides of the equation. What does this give you?\n",
            "\n",
            "\n",
            "3x = 14 + 7\n",
            "\n",
            " AI Tutor:\n",
            "It looks like there was a small mistake when adding 14 and 7. Double-check your addition and try again. What should 14 + 7 be?\n",
            "\n",
            "\n",
            "21\n",
            "\n",
            " AI Tutor:\n",
            "That's correct! Now, what is the next step to solve for x after getting 3x = 21?\n",
            "\n",
            "\n",
            "divide both sides by 3\n",
            "\n",
            " AI Tutor:\n",
            "Exactly right! What is x equal to now?\n",
            "\n",
            "\n",
            "7\n",
            "\n",
            " AI Tutor:\n",
            "Great work! You have successfully solved the equation. If you have any more questions or need further assistance, feel free to ask!\n",
            "\n",
            "\n",
            "exit\n",
            "AI Tutor: Exiting the program!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, we have created a simple AI tutor application - it is an initial version and is likely to make many mistakes. Feel free to test it with examples/tasks of varying complexity levels - you will notice that it makes mistakes, such as revealing the right answer, making factual errors, etc. Be aware that there is no single right solution to this - there are many variables you can play with - better system instructions, more examples (better few-shot prompting), other prompting hacks, and model fine-tuning (if we have enough sample data). We will get back to applying advanced prompt engineering techniques and fine-tuning eventually in the course. For now, attempt the following exercise to observe the mistakes it makes and how nuanced prompt design techniques affect ChatGPT's responses.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLMQZ51B9QUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### Exercise: Challenge the AI Tutor with Complex Tasks and Try to Improve Its Performance\n",
        "* Challenge the AI tutor with more complex math problems and observe how it responds. You can find some [math problems here](https://www.learncbse.in/extra-questions-for-class-8-maths/). One example you can try is:\n",
        " -  \"*There is a set of 10 cards numbered [6, 5, 3, 9, 7, 6, 4, 2, 8, 2]. Alice randomly picks one card from the set. What is the probability of the card being greater than 5?*\"\n",
        "* Intentionally provide incorrect facts or analysis to the AI tutor and observe if it corrects the mistake\n",
        "* Try to modify the program so it can improve on its mistakes - you can modify the system message, provide more examples in `messages`, or do something even more creative (think, you will find some non-obvious solutions)!"
      ],
      "metadata": {
        "id": "SwvmFC4YcTBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counting Tokens and Computing API Cost\n",
        "\n",
        "When you develop and deploy applications, it is important to track and monitor the API costs (since ChatGPT models are charged on a per token basis). There is a Python package called `tiktoken` created by OpenAI to compute the number of tokens used per API call.\n",
        "\n",
        "Let's first look at tokens - ChatGPT models see text in the form of tokens - it converts natural language (any language -- English, Mandarin, Spanish) into tokens.\n",
        "\n",
        "For e.g. ChatGPT will tokenize the string `\"Nelson, my neighbour, loves building AI applications!\"` into `['N','elson',',',' my',' neighbour', ',',\n",
        "' loves',' building',' AI',' applications','!']`. In this example, there are 7 words (plus punctuations) in the sentence and there are 11 tokens.\n",
        "\n",
        "<br>\n",
        "A general rule of thumb is **75 words = ~100 tokens**. For quick reference, it is helpful that one page of Google doc or Microsoft Word is about 500 words (~700 tokens).\n",
        "<br>\n",
        "\n",
        "You can [use the Tokenizer here](https://platform.openai.com/tokenizer) to see the number of tokens any given piece of text corresponds to."
      ],
      "metadata": {
        "id": "DgIrX41FhxXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple API call\n",
        "chat_response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI tutor that assists school students with math homework problems.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Help me solve the equation 3x - 9 = 21.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Try moving the 9 to the right hand side of the equation. What do you get?\"},\n",
        "        {\"role\": \"user\", \"content\": \"3x = 12\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(chat_response)"
      ],
      "metadata": {
        "id": "D-8tcRW-6Y8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55505db5-98c8-4d0c-9161-a108d8e237f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-92bcJ4imuLgFhjT6yekxRb6IQjIxK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Now, to solve for x, divide both sides of the equation by 3. What is the value of x?', role='assistant', function_call=None, tool_calls=None))], created=1710408283, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_4f0b692a78', usage=CompletionUsage(completion_tokens=24, prompt_tokens=72, total_tokens=96))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxJkraXAEDmo",
        "outputId": "f963ab78-20fd-488f-fd98-5d7d6a145f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-92bcJ4imuLgFhjT6yekxRb6IQjIxK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Now, to solve for x, divide both sides of the equation by 3. What is the value of x?', role='assistant', function_call=None, tool_calls=None))], created=1710408283, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_4f0b692a78', usage=CompletionUsage(completion_tokens=24, prompt_tokens=72, total_tokens=96))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response.usage.total_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92SOYxSuDv4U",
        "outputId": "1120a792-d77f-456a-e7f7-1a15dca6501b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_response_with_num_tokens(messages):\n",
        "\n",
        "    chat_response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages\n",
        "    )\n",
        "\n",
        "    content = chat_response.choices[0].message.content\n",
        "\n",
        "    token_count = {\n",
        "    'prompt_tokens':chat_response.usage.prompt_tokens,\n",
        "    'completion_tokens':chat_response.usage.completion_tokens,\n",
        "    'total_tokens':chat_response.usage.total_tokens,\n",
        "    }\n",
        "\n",
        "    return content, token_count"
      ],
      "metadata": {
        "id": "sPST8mDT6mRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI tutor that assists school students with math homework problems.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Help me solve the equation 3x - 9 = 21.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Try moving the 9 to the right hand side of the equation. What do you get?\"},\n",
        "        {\"role\": \"user\", \"content\": \"3x = 12\"}\n",
        "      ]\n",
        "\n",
        "chat_response_with_num_tokens(messages)"
      ],
      "metadata": {
        "id": "0BvUpyEx6rAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae53c6f-052b-4aad-fd05-558a5198d8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Now, to solve for x, divide both sides of the equation by 3. What is the value of x?',\n",
              " {'prompt_tokens': 72, 'completion_tokens': 24, 'total_tokens': 96})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets calculate the number of tokens in this message_history\n",
        "message_history = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Help me solve the equation 3x - 9 = 21.\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"Sure! Try moving the 9 to the right hand side of the equation. What do you get?\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"3x = 12\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"Well, there seems to be a mistake. When you move 9 to the right hand side, you need to change its sign. Can you try again?\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"3x = 30\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"That looks good, great job! Now, try to divide both sides by 3. What do you get?\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"x = 10\"},\n",
        "        {\"role\": \"user\", \"content\": \"Help me solve the equation x - 10 = 2x\"}]\n",
        "\n",
        "chat_response_with_num_tokens(message_history)"
      ],
      "metadata": {
        "id": "Ue37DuWj6tUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432771c2-3231-49e4-af24-ac6aff7a8fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Sure! To solve the equation \\\\(x - 10 = 2x\\\\), we'll want to get all the terms with \\\\(x\\\\) on one side of the equation. \\n\\nCan you think of a step we could take to get closer to isolating \\\\(x\\\\)?\",\n",
              " {'prompt_tokens': 252, 'completion_tokens': 56, 'total_tokens': 308})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cost of API Calls**\n",
        "Let's now calculate the cost of making API calls. We can see the [cost of various APIs here](https://openai.com/pricing). Our AI tutor program runs on gpt-3.5-turbo the cost for which is $0.0015 / 1k tokens.\n",
        "\n",
        "\n",
        "The number of tokens in the `message_history` above is ~250. But be aware that as the chat progresses, the `message_history` keeps getting longer (since we append `user_response` and `chat_response` to the `message_history` after each conversation).\n",
        "\n",
        "Let's now modify our mini program to include token and cost calculation so that we can track the usage."
      ],
      "metadata": {
        "id": "ycBMSSdQN9gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AI tutor mini program with cost calculation\n",
        "# Enter \"exit\" to terminate the program\n",
        "\n",
        "# One user message + one assistant message is one converation\n",
        "max_conversations = 20\n",
        "conversation_length = 0 # initialize\n",
        "\n",
        "#Let's also keep a track of tokens this time. Initialise an empty dictionary to count tokens\n",
        "token_count = {'prompt_tokens':0,\n",
        "               'completion_tokens':0,\n",
        "               'total_tokens':0,\n",
        "              }\n",
        "\n",
        "# initialize system message, user and assistant examples\n",
        "message_history = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Help me solve the equation 3x - 9 = 21.\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"Sure! Try moving the 9 to the right hand side of the equation. What do you get?\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"3x = 12\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"Well, there seems to be a mistake. When you move 9 to the right hand side, you need to change its sign. Can you try again?\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"3x = 30\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"That looks good, great job! Now, try to divide both sides by 3. What do you get?\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"x = 10\"}]\n",
        "\n",
        "while conversation_length < max_conversations:\n",
        "  user_input = input()\n",
        "\n",
        "  # exit if user enters exit\n",
        "  if \"exit\" in user_input.lower():\n",
        "    print(\"AI Tutor: Exiting the program!\")\n",
        "    break\n",
        "\n",
        "  message_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "  chat_response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages = message_history)\n",
        "\n",
        "  response = chat_response.choices[0].message.content\n",
        "\n",
        "  token_count['prompt_tokens'] += chat_response.usage.prompt_tokens\n",
        "  token_count['completion_tokens'] += chat_response.usage.completion_tokens\n",
        "  token_count['total_tokens'] += chat_response.usage.total_tokens\n",
        "\n",
        "  print(\"\\n\", \"AI Tutor:\")\n",
        "  print(response)\n",
        "  print(\"\\n\")\n",
        "\n",
        "  message_history.append({\"role\": \"assistant\", \"content\": response}) # add API response to message history\n",
        "  conversation_length = conversation_length + 1 # another conversation done\n",
        "\n",
        "print(token_count)"
      ],
      "metadata": {
        "id": "dCQQShTa68xT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a137c18e-fd2a-4c53-a8cf-f19b34561f73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "help me solve 3x - 14 = 7\n",
            "\n",
            " AI Tutor:\n",
            "To solve the equation 3x - 14 = 7, you want to isolate the variable x. \n",
            "\n",
            "Can you start by adding 14 to both sides of the equation to get closer to isolating x? What does the equation look like after you do this?\n",
            "\n",
            "\n",
            "3x = 21\n",
            "\n",
            " AI Tutor:\n",
            "You're on the right track! Now, can you finish solving for x by dividing both sides by 3? What is the value of x?\n",
            "\n",
            "\n",
            "x = 7\n",
            "\n",
            " AI Tutor:\n",
            "Great job! You correctly solved for x in the equation 3x - 14 = 7. Keep practicing, and don't hesitate to reach out if you have any more questions!\n",
            "\n",
            "\n",
            "exit\n",
            "AI Tutor: Exiting the program!\n",
            "{'prompt_tokens': 931, 'completion_tokens': 123, 'total_tokens': 1054}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise cost per tokens\n",
        "cost_per_1k_tokens = 0.0015\n",
        "\n",
        "# Calculate total cost by multiplying this with the total tokens calculated earlier (and also divide by 1000 since the cost is per 1000 tokens)\n",
        "conversation_cost = \"The approx total cost of this conversation is ${0}\".format(cost_per_1k_tokens*token_count['total_tokens']/1000)\n",
        "\n",
        "# Print the conversation cost\n",
        "print(conversation_cost)"
      ],
      "metadata": {
        "id": "qLnIok_XOEbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "058887bd-f98a-4572-c510-0aa66ca57d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The approx total cost of this conversation is $0.001581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setting API Usage Limits**\n",
        "\n",
        "It is highly recommended to set API usage limits on your OpenAI account to ensure that you don't exceed your monthly budget. You can [do that easily here](https://platform.openai.com/account/billing/limits).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bIpnYicWfcVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Rate limit Error**\n",
        "\n",
        "You may occassionally face rate limit error when you're making consecutive API calls to OpenAI.\n",
        "```\n",
        "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n",
        "```\n",
        "\n",
        "You may refer to the OpenAI documentation for more information on [Rate Limits](https://platform.openai.com/docs/guides/rate-limits?context=tier-free)\n",
        "\n",
        "As per OpenAI's documentation, rate limits to its API are measured in five ways:\n",
        "- RPM (requests per minute)\n",
        "- RPD (requests per day)\n",
        "- TPM (tokens per minute)\n",
        "- TPD (tokens per day)\n",
        "- IPM (images per minute)\n",
        "\n",
        "\n",
        "You may change your account's limits by visiting the [Limits](https://platform.openai.com/account/limits) in [Settings](https://platform.openai.com/account/organization) page."
      ],
      "metadata": {
        "id": "7sWqGth0yxtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One solution to fix the rate limit error is to pause the code by introducing a delay before making the subsequent API using the following command:\n",
        "\n",
        "```\n",
        "import time\n",
        "time.sleep(duration)\n",
        "```\n",
        "\n",
        "The time.sleep(duration) function in Python is part of the time module and is used to pause the execution of a program or script for a specified amount of time. It is often employed to introduce delays or control the timing of certain operations within a program. The duration parameter represents the number of seconds for which the program should be paused.\n",
        "\n",
        "By introducing delays, you are controlling the number of requests sent to the API and introduce delays between consecutive requests to avoid hitting rate limits."
      ],
      "metadata": {
        "id": "a4mQN6DpzY9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, it's recommended to use the 'tenacity' package or another exponential backoff implementation to better manage API rate limits, as hitting the API too much too fast can trigger rate limits.\n",
        "\n",
        "Using the following function ensures you get the API response without hitting the rate limits."
      ],
      "metadata": {
        "id": "_8nn6CpPy4RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tenacity"
      ],
      "metadata": {
        "id": "FSWpB4VUyyBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae4e4c6-fc5e-4eb7-df29-35168ffd4d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (8.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Best practice\n",
        "import openai\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "\n",
        "# Retry up to 6 times with exponential backoff, starting at 1 second and maxing out at 20 seconds delay\n",
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "def get_response(user_prompt):\n",
        "    MODEL = 'gpt-3.5-turbo'\n",
        "\n",
        "    message = [{\"role\":\"user\",\"content\":user_prompt}]\n",
        "\n",
        "    chat_response = openai.chat.completions.create(\n",
        "        model = MODEL,\n",
        "        messages = message)\n",
        "\n",
        "    return chat_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "peL9WYcmzO0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bOihyteCSYzi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}